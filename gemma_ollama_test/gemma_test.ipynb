{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a839137-1009-44cd-875f-a48fa6ff4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading Gemma 3N E2B...\n",
      "‚ùå Model loading failed: Unrecognized configuration class <class 'transformers.models.gemma3n.configuration_gemma3n.Gemma3nConfig'> for this kind of AutoModel: AutoModelForVision2Seq.\n",
      "Model type should be one of BlipConfig, Blip2Config, ChameleonConfig, GitConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InstructBlipVideoConfig, Kosmos2Config, LlavaConfig, LlavaNextConfig, LlavaNextVideoConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, PaliGemmaConfig, Pix2StructConfig, Qwen2_5_VLConfig, Qwen2VLConfig, VideoLlavaConfig, VipLlavaConfig, VisionEncoderDecoderConfig.\n",
      "‚úÖ System ready! Loaded 0 sections\n",
      "ü§ñ Testing text...\n",
      "üéØ Processing: What is irrigation?\n",
      "‚ö†Ô∏è No processor found, returning placeholder response\n",
      "Answer: I'm a placeholder response since the model processor isn't available yet....\n",
      "\n",
      "üìñ Testing page with images...\n",
      "üéØ Processing: In page 18, explain about sowing\n",
      "Answer: No content found for page 18...\n",
      "Modalities: []\n"
     ]
    }
   ],
   "source": [
    "# Gemma 3N E2B Multimodal System - Text, Images, Audio\n",
    "# pip install transformers torch accelerate pillow\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "def do_gemma_3n_inference(model, messages, max_new_tokens=256, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Inference function for Gemma 3N with multimodal support\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the processor from the model\n",
    "        processor = model.processor if hasattr(model, 'processor') else None\n",
    "        \n",
    "        # If no processor available, use a simple text response\n",
    "        if not processor:\n",
    "            print(\"‚ö†Ô∏è No processor found, returning placeholder response\")\n",
    "            return \"I'm a placeholder response since the model processor isn't available yet.\"\n",
    "        \n",
    "        # Process the messages\n",
    "        # Extract text and images from messages\n",
    "        text_parts = []\n",
    "        images = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if message.get(\"role\") == \"user\":\n",
    "                content = message.get(\"content\", [])\n",
    "                for item in content:\n",
    "                    if item.get(\"type\") == \"text\":\n",
    "                        text_parts.append(item.get(\"text\", \"\"))\n",
    "                    elif item.get(\"type\") == \"image\":\n",
    "                        # Load image if it's a path\n",
    "                        img_path = item.get(\"image\")\n",
    "                        if img_path and Path(img_path).exists():\n",
    "                            try:\n",
    "                                image = Image.open(img_path)\n",
    "                                images.append(image)\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ö†Ô∏è Could not load image {img_path}: {e}\")\n",
    "        \n",
    "        # Combine text parts\n",
    "        prompt = \" \".join(text_parts)\n",
    "        \n",
    "        # Process inputs\n",
    "        if images:\n",
    "            # Process with images\n",
    "            inputs = processor(\n",
    "                text=prompt,\n",
    "                images=images,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        else:\n",
    "            # Text only\n",
    "            inputs = processor(\n",
    "                text=prompt,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Move inputs to the same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Remove the original prompt from response if it's included\n",
    "        if prompt in response:\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Inference error: {e}\")\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "class Gemma3NEducationSystem:\n",
    "    def __init__(self, base_path: str = \"../datasets/education\"):\n",
    "        \"\"\"Initialize Gemma 3N multimodal education system\"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        \n",
    "        # Load structured data\n",
    "        self.structured_data = self._load_json(\"education_structured_data_extract.json\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self._load_model()\n",
    "        \n",
    "        print(f\"‚úÖ System ready! Loaded {len(self.structured_data)} sections\")\n",
    "    \n",
    "    def _load_json(self, filename: str):\n",
    "        \"\"\"Load JSON file\"\"\"\n",
    "        try:\n",
    "            file_path = self.base_path / filename\n",
    "            if file_path.exists():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load Gemma 3N E2B model\"\"\"\n",
    "        print(\"üîÑ Loading Gemma 3N E2B...\")\n",
    "        \n",
    "        try:\n",
    "            model_id = \"google/gemma-3n-E2B-it\"\n",
    "            \n",
    "            self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "            self.model = AutoModelForVision2Seq.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            \n",
    "            # Use MPS if available (Mac)\n",
    "            if torch.backends.mps.is_available():\n",
    "                self.model = self.model.to(\"mps\")\n",
    "                print(\"‚úÖ Using Apple Silicon MPS\")\n",
    "            \n",
    "            print(\"‚úÖ Gemma 3N E2B loaded!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model loading failed: {e}\")\n",
    "    \n",
    "    def is_page_based_question(self, question: str) -> Tuple[bool, Optional[int]]:\n",
    "        \"\"\"Check if question mentions a specific page\"\"\"\n",
    "        page_patterns = [\n",
    "            r'(?:in|on|from|at)\\s+page\\s+(\\d+)',\n",
    "            r'page\\s+(\\d+)',\n",
    "            r'p\\.?\\s*(\\d+)',\n",
    "            r'pg\\.?\\s*(\\d+)'\n",
    "        ]\n",
    "        \n",
    "        question_lower = question.lower()\n",
    "        for pattern in page_patterns:\n",
    "            match = re.search(pattern, question_lower)\n",
    "            if match:\n",
    "                return True, int(match.group(1))\n",
    "        return False, None\n",
    "    \n",
    "    def get_page_info(self, page_number: int) -> Dict:\n",
    "        \"\"\"Get content and images for a specific page\"\"\"\n",
    "        matching_sections = []\n",
    "        all_images = []\n",
    "        \n",
    "        for section in self.structured_data:\n",
    "            if not isinstance(section, dict):\n",
    "                continue\n",
    "            \n",
    "            page_start = section.get('page_start', 0)\n",
    "            page_end = section.get('page_end', 0)\n",
    "            \n",
    "            if page_start <= page_number <= page_end:\n",
    "                matching_sections.append(section)\n",
    "                \n",
    "                # Get images for this page\n",
    "                section_images = section.get('images', [])\n",
    "                for img in section_images:\n",
    "                    if isinstance(img, dict):\n",
    "                        img_page = img.get('page', 0)\n",
    "                        img_path = img.get('path', '')\n",
    "                        if img_page == page_number and img_path:\n",
    "                            all_images.append(img_path)\n",
    "        \n",
    "        if matching_sections:\n",
    "            combined_text = \"\\n\\n\".join([\n",
    "                f\"{section.get('main_heading', '')} - {section.get('sub_heading', '')}\\n{section.get('content', '')}\"\n",
    "                for section in matching_sections\n",
    "            ])\n",
    "            \n",
    "            return {\n",
    "                'text': combined_text,\n",
    "                'images': all_images[:3],  # Max 3 images\n",
    "                'sections': matching_sections\n",
    "            }\n",
    "        \n",
    "        return {'text': f\"No content found for page {page_number}\", 'images': [], 'sections': []}\n",
    "    \n",
    "    def process_text_question(self, question: str) -> str:\n",
    "        \"\"\"Process text-only question\"\"\"\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"Question: {question}\\n\\nAnswer:\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        return do_gemma_3n_inference(self.model, messages, max_new_tokens=300)\n",
    "    \n",
    "    def process_image_question(self, question: str, image_path: str) -> str:\n",
    "        \"\"\"Process question with image\"\"\"\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": f\"Question: {question}\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        return do_gemma_3n_inference(self.model, messages, max_new_tokens=300)\n",
    "    \n",
    "    def process_audio_question(self, audio_file: str, question: str = \"What is this audio about?\") -> str:\n",
    "        \"\"\"Process audio question\"\"\"\n",
    "        messages = [{\n",
    "            \"role\": \"user\",  \n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": audio_file},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        return do_gemma_3n_inference(self.model, messages, max_new_tokens=300)\n",
    "    \n",
    "    def process_multimodal_question(self, question: str, image_paths: List[str] = None, audio_file: str = None) -> str:\n",
    "        \"\"\"Process question with multiple modalities\"\"\"\n",
    "        content = []\n",
    "        \n",
    "        # Add audio if provided\n",
    "        if audio_file:\n",
    "            content.append({\"type\": \"audio\", \"audio\": audio_file})\n",
    "        \n",
    "        # Add images if provided\n",
    "        if image_paths:\n",
    "            for img_path in image_paths:\n",
    "                content.append({\"type\": \"image\", \"image\": img_path})\n",
    "        \n",
    "        # Add text question\n",
    "        content.append({\"type\": \"text\", \"text\": question})\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": content}]\n",
    "        \n",
    "        return do_gemma_3n_inference(self.model, messages, max_new_tokens=400)\n",
    "    \n",
    "    def answer_general_question(self, question: str, audio_file: str = None) -> Dict:\n",
    "        \"\"\"Answer general question (with optional audio)\"\"\"\n",
    "        if audio_file:\n",
    "            answer = self.process_audio_question(audio_file, question)\n",
    "        else:\n",
    "            answer = self.process_text_question(question)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'type': 'general',\n",
    "            'modalities': ['audio', 'text'] if audio_file else ['text']\n",
    "        }\n",
    "    \n",
    "    def answer_page_based_question(self, question: str, page_number: int, audio_file: str = None) -> Dict:\n",
    "        \"\"\"Answer page-based question with context, images, and optional audio\"\"\"\n",
    "        # Get page info\n",
    "        page_info = self.get_page_info(page_number)\n",
    "        \n",
    "        if not page_info['text'] or 'No content found' in page_info['text']:\n",
    "            return {\n",
    "                'answer': f\"No content found for page {page_number}\",\n",
    "                'type': 'page_based',\n",
    "                'page_number': page_number\n",
    "            }\n",
    "        \n",
    "        # Build context with page content\n",
    "        context_question = f\"Context: {page_info['text']}\\n\\nQuestion: {question}\"\n",
    "        \n",
    "        # Process with multiple modalities\n",
    "        answer = self.process_multimodal_question(\n",
    "            context_question,\n",
    "            image_paths=page_info['images'],\n",
    "            audio_file=audio_file\n",
    "        )\n",
    "        \n",
    "        modalities = ['text']\n",
    "        if page_info['images']:\n",
    "            modalities.append('images')\n",
    "        if audio_file:\n",
    "            modalities.append('audio')\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'type': 'page_based',\n",
    "            'page_number': page_number,\n",
    "            'modalities': modalities,\n",
    "            'images_used': len(page_info['images'])\n",
    "        }\n",
    "    \n",
    "    def process_question(self, question: str, audio_file: str = None) -> Dict:\n",
    "        \"\"\"Main processing method\"\"\"\n",
    "        print(f\"üéØ Processing: {question}\")\n",
    "        if audio_file:\n",
    "            print(f\"üé§ With audio: {audio_file}\")\n",
    "        \n",
    "        # Check question type\n",
    "        is_page_based, page_number = self.is_page_based_question(question)\n",
    "        \n",
    "        if is_page_based and page_number:\n",
    "            return self.answer_page_based_question(question, page_number, audio_file)\n",
    "        else:\n",
    "            return self.answer_general_question(question, audio_file)\n",
    "\n",
    "# Test function\n",
    "def test_all_modalities():\n",
    "    \"\"\"Test text, image, and audio\"\"\"\n",
    "    system = Gemma3NEducationSystem(\"../datasets/education\")\n",
    "    \n",
    "    # Test text only\n",
    "    print(\"ü§ñ Testing text...\")\n",
    "    result1 = system.process_question(\"What is irrigation?\")\n",
    "    print(f\"Answer: {result1['answer'][:100]}...\")\n",
    "    \n",
    "    # Test page-based with images\n",
    "    print(\"\\nüìñ Testing page with images...\")\n",
    "    result2 = system.process_question(\"In page 18, explain about sowing\")\n",
    "    print(f\"Answer: {result2['answer'][:100]}...\")\n",
    "    print(f\"Modalities: {result2.get('modalities', [])}\")\n",
    "    \n",
    "    # Test with audio (if you have an audio file)\n",
    "    # print(\"\\nüé§ Testing with audio...\")\n",
    "    # result3 = system.process_question(\"Explain what you heard\", audio_file=\"test_audio.mp3\")\n",
    "    # print(f\"Answer: {result3['answer'][:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_modalities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd24f7e-9222-428c-9149-6f5b6127fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
