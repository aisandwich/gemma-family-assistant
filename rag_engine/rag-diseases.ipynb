{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb289a86-cecd-4c67-99c4-2a7b54c4dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gemma3n_comp/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Rice Disease Domain RAG Builder\n",
      "Building FAISS index specifically for rice diseases...\n",
      "🔄 Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Embedding dimension: 384\n",
      "\n",
      "============================================================\n",
      "🌾 BUILDING RICE DISEASE RAG INDEX\n",
      "============================================================\n",
      "📂 Loading rice disease data from: ../datasets/rice_diseases/rice_diseases_structured_data_extract.json\n",
      "✅ Loaded 69 rice disease sections\n",
      "🔄 Processing 69 rice disease sections...\n",
      "✅ Created 262 chunks from 69 sections\n",
      "📊 Chunk type breakdown:\n",
      "   disease_name: 69 chunks\n",
      "   heading: 69 chunks\n",
      "   content: 79 chunks\n",
      "   image_reference: 45 chunks\n",
      "🦠 Diseases covered: 11\n",
      "   (Mycoplasma Like Organism), Bacterial leaf blight, Bacterial leaf streak, Blast, Brown Spot, False Smut, Grain Discoloration, Grassy stunt virus, Sheath Blight, Sheath Rot, Tungro Disease\n",
      "🔄 Generating embeddings for 262 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229615a68e5d4b93924fb06a12afe5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated embeddings shape: (262, 384)\n",
      "🔄 Building FAISS index...\n",
      "✅ FAISS index built with 262 vectors\n",
      "💾 Saving RAG index to: ../datasets/rice_diseases/rag_index\n",
      "✅ Saved FAISS index: ../datasets/rice_diseases/rag_index/faiss_index.bin\n",
      "✅ Saved embeddings: ../datasets/rice_diseases/rag_index/embeddings.npy\n",
      "✅ Saved chunks metadata: ../datasets/rice_diseases/rag_index/chunks_metadata.json\n",
      "✅ Saved configuration: ../datasets/rice_diseases/rag_index/rag_config.json\n",
      "📊 Total RAG index size: 0.92 MB\n",
      "\n",
      "🎉 Rice Disease RAG index completed!\n",
      "📂 Saved to: ../datasets/rice_diseases/rag_index\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔍 SAMPLE CHUNKS INSPECTION\n",
      "==================================================\n",
      "Total chunks: 262\n",
      "\n",
      "🦠 DISEASE_NAME samples:\n",
      "  Sample 1:\n",
      "    Text: Disease: Blast (caused by Pyricularia oryzae)...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Disease: Blast (caused by Pyricularia oryzae)...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "🦠 HEADING samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms - Leaf Blast...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "🦠 CONTENT samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms: All aboveground parts of the rice plant (leaves, leaf collar, culm, culm nodes, ne...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms: nodal infection causes the culm to break at the infected node Internodal infection...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "🦠 IMAGE_REFERENCE samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms: Image showing 2_Neck_Blast_1.png...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms: Image showing 2_Colony_of_Pyricularia_3.png...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "✅ Rice Disease RAG index ready!\n",
      "Now Surya can get accurate disease diagnosis and treatment advice!\n"
     ]
    }
   ],
   "source": [
    "# Rice Disease Domain RAG Builder\n",
    "# Build FAISS index specifically for rice disease domain\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class RiceDiseaseRAGBuilder:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize Rice Disease RAG Builder\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace sentence transformer model\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Loading embedding model: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"✅ Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def load_rice_disease_data(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load rice disease structured data from JSON file\"\"\"\n",
    "        print(f\"📂 Loading rice disease data from: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            print(f\"✅ Loaded {len(data)} rice disease sections\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"❌ Expected list format for rice disease data\")\n",
    "            return []\n",
    "    \n",
    "    def create_rice_disease_chunks(self, sections: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create text chunks from rice disease data\n",
    "        \n",
    "        Args:\n",
    "            sections: List of rice disease sections\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text, metadata, and disease info\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        print(f\"🔄 Processing {len(sections)} rice disease sections...\")\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            if not isinstance(section, dict):\n",
    "                continue\n",
    "            \n",
    "            # Extract disease information\n",
    "            disease = section.get('disease', 'Unknown')\n",
    "            causal_organism = section.get('causal_organism', '')\n",
    "            main_heading = section.get('main_heading', '')\n",
    "            sub_heading = section.get('sub_heading', '')\n",
    "            pages = section.get('pages', [])\n",
    "            source = section.get('source', 'Rice Diseases')\n",
    "            \n",
    "            # Create base metadata for this section\n",
    "            base_metadata = {\n",
    "                \"domain\": \"rice_diseases\",\n",
    "                \"section_id\": section_idx,\n",
    "                \"disease\": disease,\n",
    "                \"causal_organism\": causal_organism,\n",
    "                \"main_heading\": main_heading,\n",
    "                \"sub_heading\": sub_heading,\n",
    "                \"pages\": pages,\n",
    "                \"source\": source\n",
    "            }\n",
    "            \n",
    "            # 1. Process disease name as a chunk\n",
    "            if disease and len(disease.strip()) > 2:\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": f\"Disease: {disease} (caused by {causal_organism})\",\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"disease_name\",\n",
    "                        \"content_type\": \"disease_info\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 2. Process main heading\n",
    "            if main_heading and len(main_heading.strip()) > 2:\n",
    "                heading_text = f\"{disease} - {main_heading}\"\n",
    "                if sub_heading:\n",
    "                    heading_text += f\" - {sub_heading}\"\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": heading_text,\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"heading\",\n",
    "                        \"content_type\": \"heading\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 3. Process main content\n",
    "            content = section.get('content', '')\n",
    "            if content and len(content.strip()) > 20:\n",
    "                # Create context for content\n",
    "                context_parts = [disease]\n",
    "                if main_heading:\n",
    "                    context_parts.append(main_heading)\n",
    "                if sub_heading:\n",
    "                    context_parts.append(sub_heading)\n",
    "                \n",
    "                context_prefix = \" - \".join(context_parts)\n",
    "                context_text = f\"{context_prefix}: {content}\".strip()\n",
    "                \n",
    "                # Split long content into smaller chunks if needed\n",
    "                max_chunk_size = 800  # Larger for disease descriptions\n",
    "                if len(content) > max_chunk_size:\n",
    "                    # Split into smaller chunks\n",
    "                    sentences = content.split('. ')\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                    \n",
    "                    for sentence in sentences:\n",
    "                        sentence = sentence.strip()\n",
    "                        if not sentence:\n",
    "                            continue\n",
    "                            \n",
    "                        sentence_len = len(sentence)\n",
    "                        if current_length + sentence_len > max_chunk_size and current_chunk:\n",
    "                            # Save current chunk\n",
    "                            chunk_text = '. '.join(current_chunk)\n",
    "                            if not chunk_text.endswith('.'):\n",
    "                                chunk_text += '.'\n",
    "                            full_context = f\"{context_prefix}: {chunk_text}\"\n",
    "                            \n",
    "                            chunks.append({\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"text\": full_context,\n",
    "                                \"metadata\": {\n",
    "                                    **base_metadata,\n",
    "                                    \"chunk_type\": \"content\",\n",
    "                                    \"content_type\": \"text\",\n",
    "                                    \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                                }\n",
    "                            })\n",
    "                            chunk_id += 1\n",
    "                            \n",
    "                            # Reset for next chunk\n",
    "                            current_chunk = [sentence]\n",
    "                            current_length = sentence_len\n",
    "                        else:\n",
    "                            current_chunk.append(sentence)\n",
    "                            current_length += sentence_len + 2  # +2 for '. '\n",
    "                    \n",
    "                    # Save remaining chunk\n",
    "                    if current_chunk:\n",
    "                        chunk_text = '. '.join(current_chunk)\n",
    "                        if not chunk_text.endswith('.'):\n",
    "                            chunk_text += '.'\n",
    "                        full_context = f\"{context_prefix}: {chunk_text}\"\n",
    "                        \n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": full_context,\n",
    "                            \"metadata\": {\n",
    "                                **base_metadata,\n",
    "                                \"chunk_type\": \"content\",\n",
    "                                \"content_type\": \"text\",\n",
    "                                \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                else:\n",
    "                    # Single chunk for shorter content\n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"text\": context_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_type\": \"content\",\n",
    "                            \"content_type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            # 4. Process images\n",
    "            images = section.get('images', [])\n",
    "            for img_idx, image_name in enumerate(images):\n",
    "                if image_name and len(image_name.strip()) > 3:\n",
    "                    # Create image reference chunk\n",
    "                    image_text = f\"{disease} - {main_heading}: Image showing {image_name}\"\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"text\": image_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_type\": \"image_reference\",\n",
    "                            \"content_type\": \"image\",\n",
    "                            \"image_name\": image_name,\n",
    "                            \"image_index\": img_idx\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "        \n",
    "        print(f\"✅ Created {len(chunks)} chunks from {len(sections)} sections\")\n",
    "        \n",
    "        # Print chunk type summary\n",
    "        chunk_types = {}\n",
    "        diseases = set()\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "            diseases.add(chunk[\"metadata\"][\"disease\"])\n",
    "        \n",
    "        print(\"📊 Chunk type breakdown:\")\n",
    "        for chunk_type, count in chunk_types.items():\n",
    "            print(f\"   {chunk_type}: {count} chunks\")\n",
    "        \n",
    "        print(f\"🦠 Diseases covered: {len(diseases)}\")\n",
    "        print(f\"   {', '.join(sorted(diseases))}\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(f\"🔄 Generating embeddings for {len(texts)} chunks...\")\n",
    "        \n",
    "        # Generate embeddings in batches for efficiency\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts, \n",
    "            batch_size=32, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Build FAISS index from embeddings\"\"\"\n",
    "        print(\"🔄 Building FAISS index...\")\n",
    "        \n",
    "        # Use IndexFlatL2 for exact search\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"✅ FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def save_rag_index(self, index: faiss.Index, chunks: List[Dict[str, Any]], \n",
    "                      embeddings: np.ndarray, output_dir: str):\n",
    "        \"\"\"Save complete RAG index to disk\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"💾 Saving RAG index to: {output_dir}\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss_path = output_path / \"faiss_index.bin\"\n",
    "        faiss.write_index(index, str(faiss_path))\n",
    "        print(f\"✅ Saved FAISS index: {faiss_path}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = output_path / \"embeddings.npy\"\n",
    "        np.save(str(embeddings_path), embeddings)\n",
    "        print(f\"✅ Saved embeddings: {embeddings_path}\")\n",
    "        \n",
    "        # Save chunk metadata\n",
    "        metadata_path = output_path / \"chunks_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Saved chunks metadata: {metadata_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"domain\": \"rice_diseases\",\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"index_type\": \"IndexFlatL2\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"model_name\": str(self.embedding_model),\n",
    "            \"chunk_types\": list(set(chunk[\"metadata\"][\"chunk_type\"] for chunk in chunks)),\n",
    "            \"diseases\": list(set(chunk[\"metadata\"][\"disease\"] for chunk in chunks))\n",
    "        }\n",
    "        \n",
    "        config_path = output_path / \"rag_config.json\"\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"✅ Saved configuration: {config_path}\")\n",
    "        \n",
    "        # Calculate and display sizes\n",
    "        total_size = sum(f.stat().st_size for f in output_path.iterdir() if f.is_file())\n",
    "        print(f\"📊 Total RAG index size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    def build_rice_disease_rag(self, structured_data_path: str, output_dir: str = \"../datasets/rice_diseases/rag_index\"):\n",
    "        \"\"\"Build complete RAG index for rice disease domain\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🌾 BUILDING RICE DISEASE RAG INDEX\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load rice disease data\n",
    "        sections = self.load_rice_disease_data(structured_data_path)\n",
    "        \n",
    "        if not sections:\n",
    "            print(\"❌ No sections loaded. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Create text chunks with disease information\n",
    "        chunks = self.create_rice_disease_chunks(sections)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"❌ No chunks created. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        # Save everything\n",
    "        self.save_rag_index(index, chunks, embeddings, output_dir)\n",
    "        \n",
    "        print(f\"\\n🎉 Rice Disease RAG index completed!\")\n",
    "        print(f\"📂 Saved to: {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "# Test the RAG builder\n",
    "def test_rice_disease_rag():\n",
    "    \"\"\"Test building RAG for rice disease domain\"\"\"\n",
    "    \n",
    "    # Initialize builder\n",
    "    builder = RiceDiseaseRAGBuilder()\n",
    "    \n",
    "    # Build RAG index\n",
    "    structured_data_path = \"../datasets/rice_diseases/rice_diseases_structured_data_extract.json\"\n",
    "    output_dir = \"../datasets/rice_diseases/rag_index\"\n",
    "    \n",
    "    if Path(structured_data_path).exists():\n",
    "        builder.build_rice_disease_rag(structured_data_path, output_dir)\n",
    "    else:\n",
    "        print(f\"❌ Rice disease data file not found: {structured_data_path}\")\n",
    "        print(\"Please ensure the file exists before building RAG index.\")\n",
    "\n",
    "# Sample chunk inspector\n",
    "def inspect_sample_chunks(output_dir: str = \"../datasets/rice_diseases/rag_index\"):\n",
    "    \"\"\"Inspect some sample chunks after building\"\"\"\n",
    "    chunks_file = Path(output_dir) / \"chunks_metadata.json\"\n",
    "    \n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        \n",
    "        print(f\"\\n🔍 SAMPLE CHUNKS INSPECTION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        \n",
    "        # Show first few chunks of each type\n",
    "        chunk_types = {}\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            if chunk_type not in chunk_types:\n",
    "                chunk_types[chunk_type] = []\n",
    "            if len(chunk_types[chunk_type]) < 2:  # Show 2 examples per type\n",
    "                chunk_types[chunk_type].append(chunk)\n",
    "        \n",
    "        for chunk_type, samples in chunk_types.items():\n",
    "            print(f\"\\n🦠 {chunk_type.upper()} samples:\")\n",
    "            for i, chunk in enumerate(samples, 1):\n",
    "                print(f\"  Sample {i}:\")\n",
    "                print(f\"    Text: {chunk['text'][:100]}...\")\n",
    "                print(f\"    Disease: {chunk['metadata']['disease']}\")\n",
    "                print(f\"    Section: {chunk['metadata']['main_heading']}\")\n",
    "    else:\n",
    "        print(f\"❌ Chunks file not found: {chunks_file}\")\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "print(\"🚀 Rice Disease Domain RAG Builder\")\n",
    "print(\"Building FAISS index specifically for rice diseases...\")\n",
    "\n",
    "# Build rice disease RAG index\n",
    "test_rice_disease_rag()\n",
    "\n",
    "# Inspect the results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "inspect_sample_chunks()\n",
    "\n",
    "print(\"\\n✅ Rice Disease RAG index ready!\")\n",
    "print(\"Now Surya can get accurate disease diagnosis and treatment advice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62b6c4-f155-44bc-a186-4916c1c8aada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
