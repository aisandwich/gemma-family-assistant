{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb289a86-cecd-4c67-99c4-2a7b54c4dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gemma3n_comp/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Rice Disease Domain RAG Builder\n",
      "Building FAISS index specifically for rice diseases...\n",
      "ğŸ”„ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ… Embedding dimension: 384\n",
      "\n",
      "============================================================\n",
      "ğŸŒ¾ BUILDING RICE DISEASE RAG INDEX\n",
      "============================================================\n",
      "ğŸ“‚ Loading rice disease data from: ../datasets/rice_diseases/rice_diseases_structured_data_extract.json\n",
      "âœ… Loaded 69 rice disease sections\n",
      "ğŸ”„ Processing 69 rice disease sections...\n",
      "âœ… Created 262 chunks from 69 sections\n",
      "ğŸ“Š Chunk type breakdown:\n",
      "   disease_name: 69 chunks\n",
      "   heading: 69 chunks\n",
      "   content: 79 chunks\n",
      "   image_reference: 45 chunks\n",
      "ğŸ¦  Diseases covered: 11\n",
      "   (Mycoplasma Like Organism), Bacterial leaf blight, Bacterial leaf streak, Blast, Brown Spot, False Smut, Grain Discoloration, Grassy stunt virus, Sheath Blight, Sheath Rot, Tungro Disease\n",
      "ğŸ”„ Generating embeddings for 262 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229615a68e5d4b93924fb06a12afe5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings shape: (262, 384)\n",
      "ğŸ”„ Building FAISS index...\n",
      "âœ… FAISS index built with 262 vectors\n",
      "ğŸ’¾ Saving RAG index to: ../datasets/rice_diseases/rag_index\n",
      "âœ… Saved FAISS index: ../datasets/rice_diseases/rag_index/faiss_index.bin\n",
      "âœ… Saved embeddings: ../datasets/rice_diseases/rag_index/embeddings.npy\n",
      "âœ… Saved chunks metadata: ../datasets/rice_diseases/rag_index/chunks_metadata.json\n",
      "âœ… Saved configuration: ../datasets/rice_diseases/rag_index/rag_config.json\n",
      "ğŸ“Š Total RAG index size: 0.92 MB\n",
      "\n",
      "ğŸ‰ Rice Disease RAG index completed!\n",
      "ğŸ“‚ Saved to: ../datasets/rice_diseases/rag_index\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ” SAMPLE CHUNKS INSPECTION\n",
      "==================================================\n",
      "Total chunks: 262\n",
      "\n",
      "ğŸ¦  DISEASE_NAME samples:\n",
      "  Sample 1:\n",
      "    Text: Disease: Blast (caused by Pyricularia oryzae)...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Disease: Blast (caused by Pyricularia oryzae)...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "ğŸ¦  HEADING samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms - Leaf Blast...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "ğŸ¦  CONTENT samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms: All aboveground parts of the rice plant (leaves, leaf collar, culm, culm nodes, ne...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms: nodal infection causes the culm to break at the infected node Internodal infection...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "ğŸ¦  IMAGE_REFERENCE samples:\n",
      "  Sample 1:\n",
      "    Text: Blast - Symptoms: Image showing 2_Neck_Blast_1.png...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "  Sample 2:\n",
      "    Text: Blast - Symptoms: Image showing 2_Colony_of_Pyricularia_3.png...\n",
      "    Disease: Blast\n",
      "    Section: Symptoms\n",
      "\n",
      "âœ… Rice Disease RAG index ready!\n",
      "Now Surya can get accurate disease diagnosis and treatment advice!\n"
     ]
    }
   ],
   "source": [
    "# Rice Disease Domain RAG Builder\n",
    "# Build FAISS index specifically for rice disease domain\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class RiceDiseaseRAGBuilder:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize Rice Disease RAG Builder\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace sentence transformer model\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ Loading embedding model: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"âœ… Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def load_rice_disease_data(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load rice disease structured data from JSON file\"\"\"\n",
    "        print(f\"ğŸ“‚ Loading rice disease data from: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            print(f\"âœ… Loaded {len(data)} rice disease sections\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"âŒ Expected list format for rice disease data\")\n",
    "            return []\n",
    "    \n",
    "    def create_rice_disease_chunks(self, sections: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create text chunks from rice disease data\n",
    "        \n",
    "        Args:\n",
    "            sections: List of rice disease sections\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text, metadata, and disease info\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        print(f\"ğŸ”„ Processing {len(sections)} rice disease sections...\")\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            if not isinstance(section, dict):\n",
    "                continue\n",
    "            \n",
    "            # Extract disease information\n",
    "            disease = section.get('disease', 'Unknown')\n",
    "            causal_organism = section.get('causal_organism', '')\n",
    "            main_heading = section.get('main_heading', '')\n",
    "            sub_heading = section.get('sub_heading', '')\n",
    "            pages = section.get('pages', [])\n",
    "            source = section.get('source', 'Rice Diseases')\n",
    "            \n",
    "            # Create base metadata for this section\n",
    "            base_metadata = {\n",
    "                \"domain\": \"rice_diseases\",\n",
    "                \"section_id\": section_idx,\n",
    "                \"disease\": disease,\n",
    "                \"causal_organism\": causal_organism,\n",
    "                \"main_heading\": main_heading,\n",
    "                \"sub_heading\": sub_heading,\n",
    "                \"pages\": pages,\n",
    "                \"source\": source\n",
    "            }\n",
    "            \n",
    "            # 1. Process disease name as a chunk\n",
    "            if disease and len(disease.strip()) > 2:\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": f\"Disease: {disease} (caused by {causal_organism})\",\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"disease_name\",\n",
    "                        \"content_type\": \"disease_info\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 2. Process main heading\n",
    "            if main_heading and len(main_heading.strip()) > 2:\n",
    "                heading_text = f\"{disease} - {main_heading}\"\n",
    "                if sub_heading:\n",
    "                    heading_text += f\" - {sub_heading}\"\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": heading_text,\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"heading\",\n",
    "                        \"content_type\": \"heading\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 3. Process main content\n",
    "            content = section.get('content', '')\n",
    "            if content and len(content.strip()) > 20:\n",
    "                # Create context for content\n",
    "                context_parts = [disease]\n",
    "                if main_heading:\n",
    "                    context_parts.append(main_heading)\n",
    "                if sub_heading:\n",
    "                    context_parts.append(sub_heading)\n",
    "                \n",
    "                context_prefix = \" - \".join(context_parts)\n",
    "                context_text = f\"{context_prefix}: {content}\".strip()\n",
    "                \n",
    "                # Split long content into smaller chunks if needed\n",
    "                max_chunk_size = 800  # Larger for disease descriptions\n",
    "                if len(content) > max_chunk_size:\n",
    "                    # Split into smaller chunks\n",
    "                    sentences = content.split('. ')\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                    \n",
    "                    for sentence in sentences:\n",
    "                        sentence = sentence.strip()\n",
    "                        if not sentence:\n",
    "                            continue\n",
    "                            \n",
    "                        sentence_len = len(sentence)\n",
    "                        if current_length + sentence_len > max_chunk_size and current_chunk:\n",
    "                            # Save current chunk\n",
    "                            chunk_text = '. '.join(current_chunk)\n",
    "                            if not chunk_text.endswith('.'):\n",
    "                                chunk_text += '.'\n",
    "                            full_context = f\"{context_prefix}: {chunk_text}\"\n",
    "                            \n",
    "                            chunks.append({\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"text\": full_context,\n",
    "                                \"metadata\": {\n",
    "                                    **base_metadata,\n",
    "                                    \"chunk_type\": \"content\",\n",
    "                                    \"content_type\": \"text\",\n",
    "                                    \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                                }\n",
    "                            })\n",
    "                            chunk_id += 1\n",
    "                            \n",
    "                            # Reset for next chunk\n",
    "                            current_chunk = [sentence]\n",
    "                            current_length = sentence_len\n",
    "                        else:\n",
    "                            current_chunk.append(sentence)\n",
    "                            current_length += sentence_len + 2  # +2 for '. '\n",
    "                    \n",
    "                    # Save remaining chunk\n",
    "                    if current_chunk:\n",
    "                        chunk_text = '. '.join(current_chunk)\n",
    "                        if not chunk_text.endswith('.'):\n",
    "                            chunk_text += '.'\n",
    "                        full_context = f\"{context_prefix}: {chunk_text}\"\n",
    "                        \n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": full_context,\n",
    "                            \"metadata\": {\n",
    "                                **base_metadata,\n",
    "                                \"chunk_type\": \"content\",\n",
    "                                \"content_type\": \"text\",\n",
    "                                \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                else:\n",
    "                    # Single chunk for shorter content\n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"text\": context_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_type\": \"content\",\n",
    "                            \"content_type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            # 4. Process images\n",
    "            images = section.get('images', [])\n",
    "            for img_idx, image_name in enumerate(images):\n",
    "                if image_name and len(image_name.strip()) > 3:\n",
    "                    # Create image reference chunk\n",
    "                    image_text = f\"{disease} - {main_heading}: Image showing {image_name}\"\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"text\": image_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_type\": \"image_reference\",\n",
    "                            \"content_type\": \"image\",\n",
    "                            \"image_name\": image_name,\n",
    "                            \"image_index\": img_idx\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "        \n",
    "        print(f\"âœ… Created {len(chunks)} chunks from {len(sections)} sections\")\n",
    "        \n",
    "        # Print chunk type summary\n",
    "        chunk_types = {}\n",
    "        diseases = set()\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "            diseases.add(chunk[\"metadata\"][\"disease\"])\n",
    "        \n",
    "        print(\"ğŸ“Š Chunk type breakdown:\")\n",
    "        for chunk_type, count in chunk_types.items():\n",
    "            print(f\"   {chunk_type}: {count} chunks\")\n",
    "        \n",
    "        print(f\"ğŸ¦  Diseases covered: {len(diseases)}\")\n",
    "        print(f\"   {', '.join(sorted(diseases))}\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(f\"ğŸ”„ Generating embeddings for {len(texts)} chunks...\")\n",
    "        \n",
    "        # Generate embeddings in batches for efficiency\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts, \n",
    "            batch_size=32, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Build FAISS index from embeddings\"\"\"\n",
    "        print(\"ğŸ”„ Building FAISS index...\")\n",
    "        \n",
    "        # Use IndexFlatL2 for exact search\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"âœ… FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def save_rag_index(self, index: faiss.Index, chunks: List[Dict[str, Any]], \n",
    "                      embeddings: np.ndarray, output_dir: str):\n",
    "        \"\"\"Save complete RAG index to disk\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Saving RAG index to: {output_dir}\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss_path = output_path / \"faiss_index.bin\"\n",
    "        faiss.write_index(index, str(faiss_path))\n",
    "        print(f\"âœ… Saved FAISS index: {faiss_path}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = output_path / \"embeddings.npy\"\n",
    "        np.save(str(embeddings_path), embeddings)\n",
    "        print(f\"âœ… Saved embeddings: {embeddings_path}\")\n",
    "        \n",
    "        # Save chunk metadata\n",
    "        metadata_path = output_path / \"chunks_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… Saved chunks metadata: {metadata_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"domain\": \"rice_diseases\",\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"index_type\": \"IndexFlatL2\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"model_name\": str(self.embedding_model),\n",
    "            \"chunk_types\": list(set(chunk[\"metadata\"][\"chunk_type\"] for chunk in chunks)),\n",
    "            \"diseases\": list(set(chunk[\"metadata\"][\"disease\"] for chunk in chunks))\n",
    "        }\n",
    "        \n",
    "        config_path = output_path / \"rag_config.json\"\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"âœ… Saved configuration: {config_path}\")\n",
    "        \n",
    "        # Calculate and display sizes\n",
    "        total_size = sum(f.stat().st_size for f in output_path.iterdir() if f.is_file())\n",
    "        print(f\"ğŸ“Š Total RAG index size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    def build_rice_disease_rag(self, structured_data_path: str, output_dir: str = \"../datasets/rice_diseases/rag_index\"):\n",
    "        \"\"\"Build complete RAG index for rice disease domain\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸŒ¾ BUILDING RICE DISEASE RAG INDEX\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load rice disease data\n",
    "        sections = self.load_rice_disease_data(structured_data_path)\n",
    "        \n",
    "        if not sections:\n",
    "            print(\"âŒ No sections loaded. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Create text chunks with disease information\n",
    "        chunks = self.create_rice_disease_chunks(sections)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"âŒ No chunks created. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        # Save everything\n",
    "        self.save_rag_index(index, chunks, embeddings, output_dir)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Rice Disease RAG index completed!\")\n",
    "        print(f\"ğŸ“‚ Saved to: {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "# Test the RAG builder\n",
    "def test_rice_disease_rag():\n",
    "    \"\"\"Test building RAG for rice disease domain\"\"\"\n",
    "    \n",
    "    # Initialize builder\n",
    "    builder = RiceDiseaseRAGBuilder()\n",
    "    \n",
    "    # Build RAG index\n",
    "    structured_data_path = \"../datasets/rice_diseases/rice_diseases_structured_data_extract.json\"\n",
    "    output_dir = \"../datasets/rice_diseases/rag_index\"\n",
    "    \n",
    "    if Path(structured_data_path).exists():\n",
    "        builder.build_rice_disease_rag(structured_data_path, output_dir)\n",
    "    else:\n",
    "        print(f\"âŒ Rice disease data file not found: {structured_data_path}\")\n",
    "        print(\"Please ensure the file exists before building RAG index.\")\n",
    "\n",
    "# Sample chunk inspector\n",
    "def inspect_sample_chunks(output_dir: str = \"../datasets/rice_diseases/rag_index\"):\n",
    "    \"\"\"Inspect some sample chunks after building\"\"\"\n",
    "    chunks_file = Path(output_dir) / \"chunks_metadata.json\"\n",
    "    \n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        \n",
    "        print(f\"\\nğŸ” SAMPLE CHUNKS INSPECTION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        \n",
    "        # Show first few chunks of each type\n",
    "        chunk_types = {}\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            if chunk_type not in chunk_types:\n",
    "                chunk_types[chunk_type] = []\n",
    "            if len(chunk_types[chunk_type]) < 2:  # Show 2 examples per type\n",
    "                chunk_types[chunk_type].append(chunk)\n",
    "        \n",
    "        for chunk_type, samples in chunk_types.items():\n",
    "            print(f\"\\nğŸ¦  {chunk_type.upper()} samples:\")\n",
    "            for i, chunk in enumerate(samples, 1):\n",
    "                print(f\"  Sample {i}:\")\n",
    "                print(f\"    Text: {chunk['text'][:100]}...\")\n",
    "                print(f\"    Disease: {chunk['metadata']['disease']}\")\n",
    "                print(f\"    Section: {chunk['metadata']['main_heading']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Chunks file not found: {chunks_file}\")\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "print(\"ğŸš€ Rice Disease Domain RAG Builder\")\n",
    "print(\"Building FAISS index specifically for rice diseases...\")\n",
    "\n",
    "# Build rice disease RAG index\n",
    "test_rice_disease_rag()\n",
    "\n",
    "# Inspect the results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "inspect_sample_chunks()\n",
    "\n",
    "print(\"\\nâœ… Rice Disease RAG index ready!\")\n",
    "print(\"Now Surya can get accurate disease diagnosis and treatment advice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62b6c4-f155-44bc-a186-4916c1c8aada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
