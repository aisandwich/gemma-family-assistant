{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309350d5-188a-4a23-b602-f51394a82897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Education Domain RAG Builder\n",
      "Building FAISS index specifically for education with page information...\n",
      "ğŸ”„ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ… Embedding dimension: 384\n",
      "\n",
      "============================================================\n",
      "ğŸ“ BUILDING EDUCATION RAG INDEX\n",
      "============================================================\n",
      "ğŸ“‚ Loading education data from: ../datasets/education/education_structured_data_extract.json\n",
      "âœ… Loaded 137 education sections\n",
      "ğŸ”„ Processing 137 education sections...\n",
      "âœ… Created 2090 chunks from 137 sections\n",
      "ğŸ“Š Chunk type breakdown:\n",
      "   main_heading: 116 chunks\n",
      "   sub_heading: 137 chunks\n",
      "   content: 1203 chunks\n",
      "   image_caption: 634 chunks\n",
      "ğŸ”„ Generating embeddings for 2090 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee98022173141dba90e3096a2b399e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gemma3n_comp/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings shape: (2090, 384)\n",
      "ğŸ”„ Building FAISS index...\n",
      "âœ… FAISS index built with 2090 vectors\n",
      "ğŸ’¾ Saving RAG index to: ../datasets/education/rag_index\n",
      "âœ… Saved FAISS index: ../datasets/education/rag_index/faiss_index.bin\n",
      "âœ… Saved embeddings: ../datasets/education/rag_index/embeddings.npy\n",
      "âœ… Saved chunks metadata: ../datasets/education/rag_index/chunks_metadata.json\n",
      "âœ… Saved configuration: ../datasets/education/rag_index/rag_config.json\n",
      "ğŸ“Š Total RAG index size: 7.52 MB\n",
      "\n",
      "ğŸ‰ Education RAG index completed!\n",
      "ğŸ“‚ Saved to: ../datasets/education/rag_index\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ” SAMPLE CHUNKS INSPECTION\n",
      "==================================================\n",
      "Total chunks: 2090\n",
      "\n",
      "ğŸ“ MAIN_HEADING samples:\n",
      "  Sample 1:\n",
      "    Text: Main Topic: CROP PRODUCTION   AND MANAGEMENT...\n",
      "    Pages: 14-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "  Sample 2:\n",
      "    Text: Main Topic: CROP PRODUCTION   AND MANAGEMENT...\n",
      "    Pages: 15-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "\n",
      "ğŸ“ SUB_HEADING samples:\n",
      "  Sample 1:\n",
      "    Text: Subtopic: 1.1 Agricultural Practices...\n",
      "    Pages: 14-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "  Sample 2:\n",
      "    Text: Subtopic: 1.2 Basic Practices of Crop Production...\n",
      "    Pages: 15-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "\n",
      "ğŸ“ CONTENT samples:\n",
      "  Sample 1:\n",
      "    Text: CROP PRODUCTION   AND MANAGEMENT - 1.1 Agricultural Practices: Till 10,000 B.C.E. people were nomadi...\n",
      "    Pages: 14-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "  Sample 2:\n",
      "    Text: CROP PRODUCTION   AND MANAGEMENT - 1.1 Agricultural Practices: know that crops are of different type...\n",
      "    Pages: 14-15\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "\n",
      "ğŸ“ IMAGE_CAPTION samples:\n",
      "  Sample 1:\n",
      "    Text: Image Caption: Fig.1.1...\n",
      "    Pages: 15-17\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "  Sample 2:\n",
      "    Text: Image Caption: Fig.1.1...\n",
      "    Pages: 15-17\n",
      "    Section: CROP PRODUCTION   AND MANAGEMENT\n",
      "\n",
      "âœ… Education RAG index ready!\n",
      "Now you can use this index for accurate page-based image retrieval!\n"
     ]
    }
   ],
   "source": [
    "# Education Domain RAG Builder\n",
    "# Build FAISS index specifically for education domain with page information\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class EducationRAGBuilder:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize Education RAG Builder\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace sentence transformer model\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ Loading embedding model: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"âœ… Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def load_education_data(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load education structured data from JSON file\"\"\"\n",
    "        print(f\"ğŸ“‚ Loading education data from: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            print(f\"âœ… Loaded {len(data)} education sections\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"âŒ Expected list format for education data\")\n",
    "            return []\n",
    "    \n",
    "    def create_education_chunks(self, sections: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create text chunks from education data with page information\n",
    "        \n",
    "        Args:\n",
    "            sections: List of education sections\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text, metadata, and page info\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        print(f\"ğŸ”„ Processing {len(sections)} education sections...\")\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            if not isinstance(section, dict):\n",
    "                continue\n",
    "            \n",
    "            # Extract page information\n",
    "            page_start = section.get('page_start', 0)\n",
    "            page_end = section.get('page_end', 0)\n",
    "            main_heading = section.get('main_heading', 'Unknown')\n",
    "            sub_heading = section.get('sub_heading', '')\n",
    "            \n",
    "            # Create base metadata for this section\n",
    "            base_metadata = {\n",
    "                \"domain\": \"education\",\n",
    "                \"section_id\": section_idx,\n",
    "                \"page_start\": page_start,\n",
    "                \"page_end\": page_end,\n",
    "                \"main_heading\": main_heading,\n",
    "                \"sub_heading\": sub_heading\n",
    "            }\n",
    "            \n",
    "            # 1. Process main heading\n",
    "            if main_heading and len(main_heading.strip()) > 5:\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": f\"Main Topic: {main_heading}\",\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"main_heading\",\n",
    "                        \"content_type\": \"heading\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 2. Process sub heading\n",
    "            if sub_heading and len(sub_heading.strip()) > 5:\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": f\"Subtopic: {sub_heading}\",\n",
    "                    \"metadata\": {\n",
    "                        **base_metadata,\n",
    "                        \"chunk_type\": \"sub_heading\",\n",
    "                        \"content_type\": \"heading\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # 3. Process main content\n",
    "            content = section.get('content', '')\n",
    "            if content and len(content.strip()) > 20:\n",
    "                # Create context for content\n",
    "                context_text = f\"{main_heading} - {sub_heading}: {content}\".strip()\n",
    "                \n",
    "                # Split long content into smaller chunks if needed\n",
    "                max_chunk_size = 500  # characters\n",
    "                if len(content) > max_chunk_size:\n",
    "                    # Split into smaller chunks\n",
    "                    words = content.split()\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if current_length + len(word) > max_chunk_size and current_chunk:\n",
    "                            # Save current chunk\n",
    "                            chunk_text = ' '.join(current_chunk)\n",
    "                            full_context = f\"{main_heading} - {sub_heading}: {chunk_text}\"\n",
    "                            \n",
    "                            chunks.append({\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"text\": full_context,\n",
    "                                \"metadata\": {\n",
    "                                    **base_metadata,\n",
    "                                    \"chunk_type\": \"content\",\n",
    "                                    \"content_type\": \"text\",\n",
    "                                    \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                                }\n",
    "                            })\n",
    "                            chunk_id += 1\n",
    "                            \n",
    "                            # Reset for next chunk\n",
    "                            current_chunk = [word]\n",
    "                            current_length = len(word)\n",
    "                        else:\n",
    "                            current_chunk.append(word)\n",
    "                            current_length += len(word) + 1  # +1 for space\n",
    "                    \n",
    "                    # Save remaining chunk\n",
    "                    if current_chunk:\n",
    "                        chunk_text = ' '.join(current_chunk)\n",
    "                        full_context = f\"{main_heading} - {sub_heading}: {chunk_text}\"\n",
    "                        \n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": full_context,\n",
    "                            \"metadata\": {\n",
    "                                **base_metadata,\n",
    "                                \"chunk_type\": \"content\",\n",
    "                                \"content_type\": \"text\",\n",
    "                                \"chunk_part\": len([c for c in chunks if c[\"metadata\"].get(\"chunk_type\") == \"content\" and c[\"metadata\"][\"section_id\"] == section_idx]) + 1\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                else:\n",
    "                    # Single chunk for shorter content\n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"text\": context_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_type\": \"content\",\n",
    "                            \"content_type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            # 4. Process image captions\n",
    "            images = section.get('images', [])\n",
    "            for img_idx, image in enumerate(images):\n",
    "                if isinstance(image, dict):\n",
    "                    caption = image.get('caption', '')\n",
    "                    image_page = image.get('page', page_start)\n",
    "                    image_path = image.get('path', '')\n",
    "                    \n",
    "                    if caption and len(caption.strip()) > 5:\n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": f\"Image Caption: {caption}\",\n",
    "                            \"metadata\": {\n",
    "                                **base_metadata,\n",
    "                                \"chunk_type\": \"image_caption\",\n",
    "                                \"content_type\": \"caption\",\n",
    "                                \"image_page\": image_page,\n",
    "                                \"image_path\": image_path,\n",
    "                                \"image_index\": img_idx\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "        \n",
    "        print(f\"âœ… Created {len(chunks)} chunks from {len(sections)} sections\")\n",
    "        \n",
    "        # Print chunk type summary\n",
    "        chunk_types = {}\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "        \n",
    "        print(\"ğŸ“Š Chunk type breakdown:\")\n",
    "        for chunk_type, count in chunk_types.items():\n",
    "            print(f\"   {chunk_type}: {count} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(f\"ğŸ”„ Generating embeddings for {len(texts)} chunks...\")\n",
    "        \n",
    "        # Generate embeddings in batches for efficiency\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts, \n",
    "            batch_size=32, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Build FAISS index from embeddings\"\"\"\n",
    "        print(\"ğŸ”„ Building FAISS index...\")\n",
    "        \n",
    "        # Use IndexFlatL2 for exact search (good for education dataset size)\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"âœ… FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def save_rag_index(self, index: faiss.Index, chunks: List[Dict[str, Any]], \n",
    "                      embeddings: np.ndarray, output_dir: str):\n",
    "        \"\"\"Save complete RAG index to disk\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Saving RAG index to: {output_dir}\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss_path = output_path / \"faiss_index.bin\"\n",
    "        faiss.write_index(index, str(faiss_path))\n",
    "        print(f\"âœ… Saved FAISS index: {faiss_path}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = output_path / \"embeddings.npy\"\n",
    "        np.save(str(embeddings_path), embeddings)\n",
    "        print(f\"âœ… Saved embeddings: {embeddings_path}\")\n",
    "        \n",
    "        # Save chunk metadata\n",
    "        metadata_path = output_path / \"chunks_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… Saved chunks metadata: {metadata_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"domain\": \"education\",\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"index_type\": \"IndexFlatL2\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"model_name\": str(self.embedding_model),\n",
    "            \"chunk_types\": list(set(chunk[\"metadata\"][\"chunk_type\"] for chunk in chunks)),\n",
    "            \"page_range\": {\n",
    "                \"min_page\": min(chunk[\"metadata\"][\"page_start\"] for chunk in chunks if chunk[\"metadata\"][\"page_start\"] > 0),\n",
    "                \"max_page\": max(chunk[\"metadata\"][\"page_end\"] for chunk in chunks if chunk[\"metadata\"][\"page_end\"] > 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config_path = output_path / \"rag_config.json\"\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"âœ… Saved configuration: {config_path}\")\n",
    "        \n",
    "        # Calculate and display sizes\n",
    "        total_size = sum(f.stat().st_size for f in output_path.iterdir() if f.is_file())\n",
    "        print(f\"ğŸ“Š Total RAG index size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    def build_education_rag(self, structured_data_path: str, output_dir: str = \"../../datasets/education/rag_index\"):\n",
    "        \"\"\"Build complete RAG index for education domain\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“ BUILDING EDUCATION RAG INDEX\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load education data\n",
    "        sections = self.load_education_data(structured_data_path)\n",
    "        \n",
    "        if not sections:\n",
    "            print(\"âŒ No sections loaded. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Create text chunks with page information\n",
    "        chunks = self.create_education_chunks(sections)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"âŒ No chunks created. Cannot build RAG index.\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        # Save everything\n",
    "        self.save_rag_index(index, chunks, embeddings, output_dir)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Education RAG index completed!\")\n",
    "        print(f\"ğŸ“‚ Saved to: {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "# Test the RAG builder\n",
    "def test_education_rag():\n",
    "    \"\"\"Test building RAG for education domain\"\"\"\n",
    "    \n",
    "    # Initialize builder\n",
    "    builder = EducationRAGBuilder()\n",
    "    \n",
    "    # Build RAG index\n",
    "    structured_data_path = \"../datasets/education/education_structured_data_extract.json\"\n",
    "    output_dir = \"../datasets/education/rag_index\"\n",
    "    \n",
    "    if Path(structured_data_path).exists():\n",
    "        builder.build_education_rag(structured_data_path, output_dir)\n",
    "    else:\n",
    "        print(f\"âŒ Education data file not found: {structured_data_path}\")\n",
    "        print(\"Please ensure the file exists before building RAG index.\")\n",
    "\n",
    "# Sample chunk inspector\n",
    "def inspect_sample_chunks(output_dir: str = \"../datasets/education/rag_index\"):\n",
    "    \"\"\"Inspect some sample chunks after building\"\"\"\n",
    "    chunks_file = Path(output_dir) / \"chunks_metadata.json\"\n",
    "    \n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        \n",
    "        print(f\"\\nğŸ” SAMPLE CHUNKS INSPECTION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        \n",
    "        # Show first few chunks of each type\n",
    "        chunk_types = {}\n",
    "        for chunk in chunks:\n",
    "            chunk_type = chunk[\"metadata\"][\"chunk_type\"]\n",
    "            if chunk_type not in chunk_types:\n",
    "                chunk_types[chunk_type] = []\n",
    "            if len(chunk_types[chunk_type]) < 2:  # Show 2 examples per type\n",
    "                chunk_types[chunk_type].append(chunk)\n",
    "        \n",
    "        for chunk_type, samples in chunk_types.items():\n",
    "            print(f\"\\nğŸ“ {chunk_type.upper()} samples:\")\n",
    "            for i, chunk in enumerate(samples, 1):\n",
    "                print(f\"  Sample {i}:\")\n",
    "                print(f\"    Text: {chunk['text'][:100]}...\")\n",
    "                print(f\"    Pages: {chunk['metadata']['page_start']}-{chunk['metadata']['page_end']}\")\n",
    "                print(f\"    Section: {chunk['metadata']['main_heading']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Chunks file not found: {chunks_file}\")\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Education Domain RAG Builder\")\n",
    "    print(\"Building FAISS index specifically for education with page information...\")\n",
    "    \n",
    "    # Build education RAG index\n",
    "    test_education_rag()\n",
    "    \n",
    "    # Inspect the results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    inspect_sample_chunks()\n",
    "    \n",
    "    print(\"\\nâœ… Education RAG index ready!\")\n",
    "    print(\"Now you can use this index for accurate page-based image retrieval!\")\n",
    "\n",
    "# Uncomment to run\n",
    "# test_education_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f67e6-8512-4751-8929-d9c989b0880e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
