{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309350d5-188a-4a23-b602-f51394a82897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Index Builder for Gemma Family Assistant\n",
    "# Build FAISS indexes for all 5 domains from structured data\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install sentence-transformers faiss-cpu\n",
    "\n",
    "class RAGIndexBuilder:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG Index Builder\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace sentence transformer model\n",
    "        \"\"\"\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def load_structured_data(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load structured data from JSON file\"\"\"\n",
    "        print(f\"Loading structured data from: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data) if isinstance(data, list) else 'structured'} entries\")\n",
    "        return data\n",
    "    \n",
    "    def create_text_chunks(self, data: Dict[str, Any], domain: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create text chunks from structured data\n",
    "        \n",
    "        Args:\n",
    "            data: Structured data dictionary\n",
    "            domain: Domain name (ayurveda, education, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        def process_item(item, parent_context=\"\"):\n",
    "            nonlocal chunk_id\n",
    "            \n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    context = f\"{parent_context} > {key}\" if parent_context else key\n",
    "                    \n",
    "                    if isinstance(value, str) and len(value.strip()) > 20:\n",
    "                        # Create chunk for substantial text content\n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": f\"{key}: {value}\",\n",
    "                            \"metadata\": {\n",
    "                                \"domain\": domain,\n",
    "                                \"context\": context,\n",
    "                                \"key\": key,\n",
    "                                \"chunk_type\": \"text_content\"\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                    \n",
    "                    elif isinstance(value, (dict, list)):\n",
    "                        # Recursively process nested structures\n",
    "                        process_item(value, context)\n",
    "                        \n",
    "            elif isinstance(item, list):\n",
    "                for i, sub_item in enumerate(item):\n",
    "                    context = f\"{parent_context}[{i}]\" if parent_context else f\"item_{i}\"\n",
    "                    process_item(sub_item, context)\n",
    "                    \n",
    "            elif isinstance(item, str) and len(item.strip()) > 20:\n",
    "                # Direct string content\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": item,\n",
    "                    \"metadata\": {\n",
    "                        \"domain\": domain,\n",
    "                        \"context\": parent_context,\n",
    "                        \"chunk_type\": \"direct_text\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "        \n",
    "        # Process the entire data structure\n",
    "        process_item(data)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} text chunks for {domain}\")\n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "        \n",
    "        # Generate embeddings in batches for efficiency\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts, \n",
    "            batch_size=32, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Build FAISS index from embeddings\"\"\"\n",
    "        print(\"Building FAISS index...\")\n",
    "        \n",
    "        # Use IndexFlatL2 for exact search (good for small datasets)\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def save_rag_index(self, index: faiss.Index, chunks: List[Dict[str, Any]], \n",
    "                      embeddings: np.ndarray, domain: str, output_dir: str):\n",
    "        \"\"\"Save complete RAG index to disk\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss_path = os.path.join(output_dir, \"faiss_index.bin\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"Saved FAISS index: {faiss_path}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = os.path.join(output_dir, \"embeddings.npy\")\n",
    "        np.save(embeddings_path, embeddings)\n",
    "        print(f\"Saved embeddings: {embeddings_path}\")\n",
    "        \n",
    "        # Save chunk metadata\n",
    "        metadata_path = os.path.join(output_dir, \"chunks_metadata.json\")\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved chunks metadata: {metadata_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"domain\": domain,\n",
    "            \"embedding_model\": self.embedding_model.get_sentence_embedding_dimension(),\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"index_type\": \"IndexFlatL2\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"model_name\": str(self.embedding_model)\n",
    "        }\n",
    "        \n",
    "        config_path = os.path.join(output_dir, \"rag_config.json\")\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"Saved configuration: {config_path}\")\n",
    "        \n",
    "        # Calculate and display sizes\n",
    "        total_size = sum(os.path.getsize(os.path.join(output_dir, f)) \n",
    "                        for f in os.listdir(output_dir))\n",
    "        print(f\"Total RAG index size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    def build_domain_rag(self, domain: str, structured_data_path: str, datasets_dir: str = \"datasets\"):\n",
    "        \"\"\"Build complete RAG index for a domain\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Building RAG index for domain: {domain.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load structured data\n",
    "        data = self.load_structured_data(structured_data_path)\n",
    "        \n",
    "        # Create text chunks\n",
    "        chunks = self.create_text_chunks(data, domain)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(f\"No chunks created for {domain}. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        # Save everything\n",
    "        output_dir = os.path.join(datasets_dir, domain, \"rag_index\")\n",
    "        self.save_rag_index(index, chunks, embeddings, domain, output_dir)\n",
    "        \n",
    "        print(f\"‚úÖ RAG index for {domain} completed!\")\n",
    "        return output_dir\n",
    "\n",
    "def build_all_rag_indexes(datasets_dir: str = \"../datasets\"):\n",
    "    \"\"\"Build RAG indexes for all 5 domains\"\"\"\n",
    "    \n",
    "    # Initialize RAG builder\n",
    "    builder = RAGIndexBuilder()\n",
    "    \n",
    "    # Define domains and their structured data files\n",
    "    domains = {\n",
    "        \"ayurveda\": \"ayurveda_structured_data_extract.json\",\n",
    "        \"depression\": \"depression_structured_data_extract.json\", \n",
    "        \"disaster_management\": \"disaster_management_structured_data_extract.json\",\n",
    "        \"education\": \"education_structured_data_extract.json\",\n",
    "        \"rice_diseases\": \"rice_diseases_structured_data_extract.json\"\n",
    "    }\n",
    "    \n",
    "    successful_builds = []\n",
    "    failed_builds = []\n",
    "    \n",
    "    # Build RAG index for each domain\n",
    "    for domain, filename in domains.items():\n",
    "        try:\n",
    "            structured_data_path = os.path.join(datasets_dir, domain, filename)\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(structured_data_path):\n",
    "                print(f\"‚ùå File not found: {structured_data_path}\")\n",
    "                failed_builds.append(domain)\n",
    "                continue\n",
    "            \n",
    "            # Build RAG index\n",
    "            output_dir = builder.build_domain_rag(domain, structured_data_path, datasets_dir)\n",
    "            successful_builds.append(domain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error building RAG for {domain}: {str(e)}\")\n",
    "            failed_builds.append(domain)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RAG INDEX BUILD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úÖ Successfully built: {len(successful_builds)} domains\")\n",
    "    for domain in successful_builds:\n",
    "        print(f\"   - {domain}\")\n",
    "    \n",
    "    if failed_builds:\n",
    "        print(f\"‚ùå Failed builds: {len(failed_builds)} domains\")\n",
    "        for domain in failed_builds:\n",
    "            print(f\"   - {domain}\")\n",
    "    \n",
    "    print(f\"\\nüéâ RAG index building completed!\")\n",
    "\n",
    "# Example usage for testing a single domain\n",
    "def test_single_domain():\n",
    "    \"\"\"Test building RAG for a single domain\"\"\"\n",
    "    builder = RAGIndexBuilder()\n",
    "    \n",
    "    # Test with ayurveda domain (adjust path as needed)\n",
    "    domain = \"ayurveda\"\n",
    "    structured_data_path = f\"datasets/{domain}/ayurveda_structured_data_extract.json\"\n",
    "    \n",
    "    if os.path.exists(structured_data_path):\n",
    "        builder.build_domain_rag(domain, structured_data_path)\n",
    "    else:\n",
    "        print(f\"Test file not found: {structured_data_path}\")\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting RAG Index Builder for Gemma Family Assistant\")\n",
    "    print(\"Building FAISS indexes for all 5 domains...\")\n",
    "    \n",
    "    # Build all RAG indexes\n",
    "    build_all_rag_indexes()\n",
    "    \n",
    "    print(\"\\n‚úÖ All RAG indexes built and saved!\")\n",
    "    print(\"You can now use these indexes for fast retrieval in your Streamlit app.\")\n",
    "\n",
    "# Uncomment to test single domain first\n",
    "# test_single_domain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
