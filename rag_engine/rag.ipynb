{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309350d5-188a-4a23-b602-f51394a82897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gemma3n_comp/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RAG Index Builder for Gemma Family Assistant\n",
      "Building FAISS indexes for all 5 domains...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b250c0e4ef1342f7a2cf7abcfccedfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530cf65e50d44ae7bd7f57b213befd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539259c0933a40b4ad279cb5734b02a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373c0d6f2d64479b3fff0a405c7abe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c527434a54654443be1712d5fdf2d051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4fb04bef2447f4a8553fe8ad66a5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac999e7182e4712a0507cd00102f025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8c8f7665b44566aef04369876c57dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0b504a58124f59831e6793340e0e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bba77b62f144559c2c8bc48b2764f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dc8a9864ec442ea7c4b8712b7df780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "\n",
      "==================================================\n",
      "Building RAG index for domain: AYURVEDA\n",
      "==================================================\n",
      "Loading structured data from: ../datasets/ayurveda/ayurveda_structured_data_extract.json\n",
      "Loaded 25 entries\n",
      "Created 164 text chunks for ayurveda\n",
      "Generating embeddings for 164 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609ea41a29d24fd992461b494790ba46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gemma3n_comp/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (164, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 164 vectors\n",
      "Saved FAISS index: ../datasets/ayurveda/rag_index/faiss_index.bin\n",
      "Saved embeddings: ../datasets/ayurveda/rag_index/embeddings.npy\n",
      "Saved chunks metadata: ../datasets/ayurveda/rag_index/chunks_metadata.json\n",
      "Saved configuration: ../datasets/ayurveda/rag_index/rag_config.json\n",
      "Total RAG index size: 0.53 MB\n",
      "‚úÖ RAG index for ayurveda completed!\n",
      "\n",
      "==================================================\n",
      "Building RAG index for domain: DEPRESSION\n",
      "==================================================\n",
      "Loading structured data from: ../datasets/depression/depression_structured_data_extract.json\n",
      "Loaded 25 entries\n",
      "Created 81 text chunks for depression\n",
      "Generating embeddings for 81 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff173435eaf4670881810d7a1481593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (81, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 81 vectors\n",
      "Saved FAISS index: ../datasets/depression/rag_index/faiss_index.bin\n",
      "Saved embeddings: ../datasets/depression/rag_index/embeddings.npy\n",
      "Saved chunks metadata: ../datasets/depression/rag_index/chunks_metadata.json\n",
      "Saved configuration: ../datasets/depression/rag_index/rag_config.json\n",
      "Total RAG index size: 0.29 MB\n",
      "‚úÖ RAG index for depression completed!\n",
      "\n",
      "==================================================\n",
      "Building RAG index for domain: DISASTER_MANAGEMENT\n",
      "==================================================\n",
      "Loading structured data from: ../datasets/disaster_management/disaster_management_structured_data_extract.json\n",
      "Loaded 8 entries\n",
      "Created 23 text chunks for disaster_management\n",
      "Generating embeddings for 23 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb1c85384e846ba8509368d6a5b57f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (23, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 23 vectors\n",
      "Saved FAISS index: ../datasets/disaster_management/rag_index/faiss_index.bin\n",
      "Saved embeddings: ../datasets/disaster_management/rag_index/embeddings.npy\n",
      "Saved chunks metadata: ../datasets/disaster_management/rag_index/chunks_metadata.json\n",
      "Saved configuration: ../datasets/disaster_management/rag_index/rag_config.json\n",
      "Total RAG index size: 0.10 MB\n",
      "‚úÖ RAG index for disaster_management completed!\n",
      "\n",
      "==================================================\n",
      "Building RAG index for domain: EDUCATION\n",
      "==================================================\n",
      "Loading structured data from: ../datasets/education/education_structured_data_extract.json\n",
      "Loaded 137 entries\n",
      "Created 1286 text chunks for education\n",
      "Generating embeddings for 1286 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc16fcdda344d97b329b03317c4660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1286, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 1286 vectors\n",
      "Saved FAISS index: ../datasets/education/rag_index/faiss_index.bin\n",
      "Saved embeddings: ../datasets/education/rag_index/embeddings.npy\n",
      "Saved chunks metadata: ../datasets/education/rag_index/chunks_metadata.json\n",
      "Saved configuration: ../datasets/education/rag_index/rag_config.json\n",
      "Total RAG index size: 4.63 MB\n",
      "‚úÖ RAG index for education completed!\n",
      "\n",
      "==================================================\n",
      "Building RAG index for domain: RICE_DISEASES\n",
      "==================================================\n",
      "Loading structured data from: ../datasets/rice_diseases/rice_diseases_structured_data_extract.json\n",
      "Loaded 69 entries\n",
      "Created 194 text chunks for rice_diseases\n",
      "Generating embeddings for 194 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b25f8c66f34b8b8521a8121b328740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (194, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 194 vectors\n",
      "Saved FAISS index: ../datasets/rice_diseases/rag_index/faiss_index.bin\n",
      "Saved embeddings: ../datasets/rice_diseases/rag_index/embeddings.npy\n",
      "Saved chunks metadata: ../datasets/rice_diseases/rag_index/chunks_metadata.json\n",
      "Saved configuration: ../datasets/rice_diseases/rag_index/rag_config.json\n",
      "Total RAG index size: 0.64 MB\n",
      "‚úÖ RAG index for rice_diseases completed!\n",
      "\n",
      "============================================================\n",
      "RAG INDEX BUILD SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully built: 5 domains\n",
      "   - ayurveda\n",
      "   - depression\n",
      "   - disaster_management\n",
      "   - education\n",
      "   - rice_diseases\n",
      "\n",
      "üéâ RAG index building completed!\n",
      "\n",
      "‚úÖ All RAG indexes built and saved!\n",
      "You can now use these indexes for fast retrieval in your Streamlit app.\n"
     ]
    }
   ],
   "source": [
    "# RAG Index Builder for Gemma Family Assistant\n",
    "# Build FAISS indexes for all 5 domains from structured data\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install sentence-transformers faiss-cpu\n",
    "\n",
    "class RAGIndexBuilder:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG Index Builder\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace sentence transformer model\n",
    "        \"\"\"\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def load_structured_data(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load structured data from JSON file\"\"\"\n",
    "        print(f\"Loading structured data from: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data) if isinstance(data, list) else 'structured'} entries\")\n",
    "        return data\n",
    "    \n",
    "    def create_text_chunks(self, data: Dict[str, Any], domain: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create text chunks from structured data\n",
    "        \n",
    "        Args:\n",
    "            data: Structured data dictionary\n",
    "            domain: Domain name (ayurveda, education, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        def process_item(item, parent_context=\"\"):\n",
    "            nonlocal chunk_id\n",
    "            \n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    context = f\"{parent_context} > {key}\" if parent_context else key\n",
    "                    \n",
    "                    if isinstance(value, str) and len(value.strip()) > 20:\n",
    "                        # Create chunk for substantial text content\n",
    "                        chunks.append({\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"text\": f\"{key}: {value}\",\n",
    "                            \"metadata\": {\n",
    "                                \"domain\": domain,\n",
    "                                \"context\": context,\n",
    "                                \"key\": key,\n",
    "                                \"chunk_type\": \"text_content\"\n",
    "                            }\n",
    "                        })\n",
    "                        chunk_id += 1\n",
    "                    \n",
    "                    elif isinstance(value, (dict, list)):\n",
    "                        # Recursively process nested structures\n",
    "                        process_item(value, context)\n",
    "                        \n",
    "            elif isinstance(item, list):\n",
    "                for i, sub_item in enumerate(item):\n",
    "                    context = f\"{parent_context}[{i}]\" if parent_context else f\"item_{i}\"\n",
    "                    process_item(sub_item, context)\n",
    "                    \n",
    "            elif isinstance(item, str) and len(item.strip()) > 20:\n",
    "                # Direct string content\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": item,\n",
    "                    \"metadata\": {\n",
    "                        \"domain\": domain,\n",
    "                        \"context\": parent_context,\n",
    "                        \"chunk_type\": \"direct_text\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "        \n",
    "        # Process the entire data structure\n",
    "        process_item(data)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} text chunks for {domain}\")\n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "        \n",
    "        # Generate embeddings in batches for efficiency\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts, \n",
    "            batch_size=32, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Build FAISS index from embeddings\"\"\"\n",
    "        print(\"Building FAISS index...\")\n",
    "        \n",
    "        # Use IndexFlatL2 for exact search (good for small datasets)\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def save_rag_index(self, index: faiss.Index, chunks: List[Dict[str, Any]], \n",
    "                      embeddings: np.ndarray, domain: str, output_dir: str):\n",
    "        \"\"\"Save complete RAG index to disk\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss_path = os.path.join(output_dir, \"faiss_index.bin\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"Saved FAISS index: {faiss_path}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = os.path.join(output_dir, \"embeddings.npy\")\n",
    "        np.save(embeddings_path, embeddings)\n",
    "        print(f\"Saved embeddings: {embeddings_path}\")\n",
    "        \n",
    "        # Save chunk metadata\n",
    "        metadata_path = os.path.join(output_dir, \"chunks_metadata.json\")\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved chunks metadata: {metadata_path}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"domain\": domain,\n",
    "            \"embedding_model\": self.embedding_model.get_sentence_embedding_dimension(),\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"index_type\": \"IndexFlatL2\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"model_name\": str(self.embedding_model)\n",
    "        }\n",
    "        \n",
    "        config_path = os.path.join(output_dir, \"rag_config.json\")\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"Saved configuration: {config_path}\")\n",
    "        \n",
    "        # Calculate and display sizes\n",
    "        total_size = sum(os.path.getsize(os.path.join(output_dir, f)) \n",
    "                        for f in os.listdir(output_dir))\n",
    "        print(f\"Total RAG index size: {total_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    def build_domain_rag(self, domain: str, structured_data_path: str, datasets_dir: str = \"datasets\"):\n",
    "        \"\"\"Build complete RAG index for a domain\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Building RAG index for domain: {domain.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load structured data\n",
    "        data = self.load_structured_data(structured_data_path)\n",
    "        \n",
    "        # Create text chunks\n",
    "        chunks = self.create_text_chunks(data, domain)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(f\"No chunks created for {domain}. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        # Save everything\n",
    "        output_dir = os.path.join(datasets_dir, domain, \"rag_index\")\n",
    "        self.save_rag_index(index, chunks, embeddings, domain, output_dir)\n",
    "        \n",
    "        print(f\"‚úÖ RAG index for {domain} completed!\")\n",
    "        return output_dir\n",
    "\n",
    "def build_all_rag_indexes(datasets_dir: str = \"../datasets\"):\n",
    "    \"\"\"Build RAG indexes for all 5 domains\"\"\"\n",
    "    \n",
    "    # Initialize RAG builder\n",
    "    builder = RAGIndexBuilder()\n",
    "    \n",
    "    # Define domains and their structured data files\n",
    "    domains = {\n",
    "        \"ayurveda\": \"ayurveda_structured_data_extract.json\",\n",
    "        \"depression\": \"depression_structured_data_extract.json\", \n",
    "        \"disaster_management\": \"disaster_management_structured_data_extract.json\",\n",
    "        \"education\": \"education_structured_data_extract.json\",\n",
    "        \"rice_diseases\": \"rice_diseases_structured_data_extract.json\"\n",
    "    }\n",
    "    \n",
    "    successful_builds = []\n",
    "    failed_builds = []\n",
    "    \n",
    "    # Build RAG index for each domain\n",
    "    for domain, filename in domains.items():\n",
    "        try:\n",
    "            structured_data_path = os.path.join(datasets_dir, domain, filename)\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(structured_data_path):\n",
    "                print(f\"‚ùå File not found: {structured_data_path}\")\n",
    "                failed_builds.append(domain)\n",
    "                continue\n",
    "            \n",
    "            # Build RAG index\n",
    "            output_dir = builder.build_domain_rag(domain, structured_data_path, datasets_dir)\n",
    "            successful_builds.append(domain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error building RAG for {domain}: {str(e)}\")\n",
    "            failed_builds.append(domain)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RAG INDEX BUILD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úÖ Successfully built: {len(successful_builds)} domains\")\n",
    "    for domain in successful_builds:\n",
    "        print(f\"   - {domain}\")\n",
    "    \n",
    "    if failed_builds:\n",
    "        print(f\"‚ùå Failed builds: {len(failed_builds)} domains\")\n",
    "        for domain in failed_builds:\n",
    "            print(f\"   - {domain}\")\n",
    "    \n",
    "    print(f\"\\nüéâ RAG index building completed!\")\n",
    "\n",
    "# Example usage for testing a single domain\n",
    "def test_single_domain():\n",
    "    \"\"\"Test building RAG for a single domain\"\"\"\n",
    "    builder = RAGIndexBuilder()\n",
    "    \n",
    "    # Test with ayurveda domain (adjust path as needed)\n",
    "    domain = \"ayurveda\"\n",
    "    structured_data_path = f\"datasets/{domain}/ayurveda_structured_data_extract.json\"\n",
    "    \n",
    "    if os.path.exists(structured_data_path):\n",
    "        builder.build_domain_rag(domain, structured_data_path)\n",
    "    else:\n",
    "        print(f\"Test file not found: {structured_data_path}\")\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting RAG Index Builder for Gemma Family Assistant\")\n",
    "    print(\"Building FAISS indexes for all 5 domains...\")\n",
    "    \n",
    "    # Build all RAG indexes\n",
    "    build_all_rag_indexes()\n",
    "    \n",
    "    print(\"\\n‚úÖ All RAG indexes built and saved!\")\n",
    "    print(\"You can now use these indexes for fast retrieval in your Streamlit app.\")\n",
    "\n",
    "# Uncomment to test single domain first\n",
    "# test_single_domain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f67e6-8512-4751-8929-d9c989b0880e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
