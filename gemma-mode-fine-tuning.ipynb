{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:34:33.656269Z",
     "iopub.status.busy": "2025-08-04T14:34:33.655840Z",
     "iopub.status.idle": "2025-08-04T14:37:21.373556Z",
     "shell.execute_reply": "2025-08-04T14:37:21.372439Z",
     "shell.execute_reply.started": "2025-08-04T14:34:33.656234Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install transformers -U\n",
    "!pip install timm  --upgrade\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:37:21.375625Z",
     "iopub.status.busy": "2025-08-04T14:37:21.375355Z",
     "iopub.status.idle": "2025-08-04T14:37:56.289017Z",
     "shell.execute_reply": "2025-08-04T14:37:56.288461Z",
     "shell.execute_reply.started": "2025-08-04T14:37:21.375592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 14:37:29.761802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754318249.995307     116 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754318250.060537     116 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:50:46.983697Z",
     "iopub.status.busy": "2025-08-04T14:50:46.982996Z",
     "iopub.status.idle": "2025-08-04T14:50:47.058850Z",
     "shell.execute_reply": "2025-08-04T14:50:47.058239Z",
     "shell.execute_reply.started": "2025-08-04T14:50:46.983671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 JSON files:\n",
      "  - ayurveda_qa_dataset.json\n",
      "  - depression_qa_dataset.json\n",
      "  - education_qa_dataset.json\n",
      "  - rice_disease_qa_results.json\n",
      "  - disaster_management_qa_dataset.json\n",
      "\n",
      "Loading: ayurveda_qa_dataset.json\n",
      "  ‚úÖ Added 375 Q&A pairs\n",
      "\n",
      "Loading: depression_qa_dataset.json\n",
      "  ‚úÖ Added 125 Q&A pairs\n",
      "\n",
      "Loading: education_qa_dataset.json\n",
      "  ‚úÖ Added 1075 Q&A pairs\n",
      "\n",
      "Loading: rice_disease_qa_results.json\n",
      "  ‚úÖ Added 895 Q&A pairs\n",
      "\n",
      "Loading: disaster_management_qa_dataset.json\n",
      "  ‚úÖ Added 160 Q&A pairs\n",
      "\n",
      "üéâ Total Q&A pairs collected: 2630\n",
      "\n",
      "Sample Q&A:\n",
      "Q: My child is constantly complaining of stomach discomfort after meals. In Ayurveda, what home remedy can I use to help with this indigestion?\n",
      "A: In Ayurveda, Adrak (Ginger) is excellent for digestion. You can give your child 5 grams of crushed g...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Get all JSON files from the input folder\n",
    "input_folder = '/kaggle/input/data-for-fine-tuning-gemma'\n",
    "all_files = os.listdir(input_folder)\n",
    "json_files = [f for f in all_files if f.endswith('.json')]\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files:\")\n",
    "for file in json_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Combine all Q&A pairs\n",
    "all_qa_pairs = []\n",
    "\n",
    "for filename in json_files:\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    print(f\"\\nLoading: {filename}\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract Q&A pairs from each dataset\n",
    "        count = 0\n",
    "        for item in data:\n",
    "            if 'qa_pairs' in item:\n",
    "                all_qa_pairs.extend(item['qa_pairs'])\n",
    "                count += len(item['qa_pairs'])\n",
    "        \n",
    "        print(f\"  ‚úÖ Added {count} Q&A pairs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Total Q&A pairs collected: {len(all_qa_pairs)}\")\n",
    "\n",
    "# Show sample\n",
    "if all_qa_pairs:\n",
    "    print(f\"\\nSample Q&A:\")\n",
    "    print(f\"Q: {all_qa_pairs[0]['question']}\")\n",
    "    print(f\"A: {all_qa_pairs[0]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:50:50.035704Z",
     "iopub.status.busy": "2025-08-04T14:50:50.035416Z",
     "iopub.status.idle": "2025-08-04T14:50:50.041314Z",
     "shell.execute_reply": "2025-08-04T14:50:50.040542Z",
     "shell.execute_reply.started": "2025-08-04T14:50:50.035683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2630"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:50:50.446810Z",
     "iopub.status.busy": "2025-08-04T14:50:50.446235Z",
     "iopub.status.idle": "2025-08-04T14:50:50.501377Z",
     "shell.execute_reply": "2025-08-04T14:50:50.500687Z",
     "shell.execute_reply.started": "2025-08-04T14:50:50.446788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2630 Q&A pairs to conversation format\n",
      "‚úÖ Dataset created with 2630 conversations\n",
      "\n",
      "Sample conversation:\n",
      "User: My child is constantly complaining of stomach discomfort after meals. In Ayurveda, what home remedy can I use to help with this indigestion?\n",
      "Assistant: In Ayurveda, Adrak (Ginger) is excellent for digestion. You can give your child 5 grams of crushed g...\n",
      "\n",
      "Dataset info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 2630\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "conversation_data = []\n",
    "\n",
    "for qa in all_qa_pairs:\n",
    "    # Each Q&A becomes a conversation with user/assistant roles\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": qa['question']},\n",
    "        {\"role\": \"assistant\", \"content\": qa['answer']}\n",
    "    ]\n",
    "    conversation_data.append(conversation)\n",
    "\n",
    "print(f\"Converted {len(conversation_data)} Q&A pairs to conversation format\")\n",
    "\n",
    "# Create dataset like the example\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"conversations\": conversation_data})\n",
    "\n",
    "print(f\"‚úÖ Dataset created with {len(dataset)} conversations\")\n",
    "\n",
    "# Show sample conversation format\n",
    "print(f\"\\nSample conversation:\")\n",
    "print(f\"User: {conversation_data[0][0]['content']}\")\n",
    "print(f\"Assistant: {conversation_data[0][1]['content'][:100]}...\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(f\"\\nDataset info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:50:52.304592Z",
     "iopub.status.busy": "2025-08-04T14:50:52.303938Z",
     "iopub.status.idle": "2025-08-04T14:52:11.064072Z",
     "shell.execute_reply": "2025-08-04T14:52:11.063242Z",
     "shell.execute_reply.started": "2025-08-04T14:50:52.304567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08806764e6ab49999926e999b0cbc54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e371e0e79c433a8e89bc6da9cc7af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a09cdaf583f4b80a2b5f56f0ecdc5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7884590dc904f27ba96d85568953272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9263c2eacb447e84803c35b2dd03a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83dfb7a074941bc99c20b019435ad6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9695829e05e64d128dc4638b13dcb976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7603189e22e4412a8c62a2e0e318037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b4df9e193d468ca61b5c7cab1c3861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccb11fe7a46409bbf5f98b271440c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30489a26032941daa9a201b4c6df80f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9683c10196941d795005e8a409ce7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff68da8a4004f3685362a7af3ba9618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# Reload the base model\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    device_map = \"cuda:0\",\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:11.065494Z",
     "iopub.status.busy": "2025-08-04T14:52:11.065197Z",
     "iopub.status.idle": "2025-08-04T14:52:11.069894Z",
     "shell.execute_reply": "2025-08-04T14:52:11.068994Z",
     "shell.execute_reply.started": "2025-08-04T14:52:11.065476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache limit increased\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Increase cache limit\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.cache_size_limit = 256\n",
    "\n",
    "print(\"‚úÖ Cache limit increased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:11.070861Z",
     "iopub.status.busy": "2025-08-04T14:52:11.070620Z",
     "iopub.status.idle": "2025-08-04T14:52:16.160624Z",
     "shell.execute_reply": "2025-08-04T14:52:16.159981Z",
     "shell.execute_reply.started": "2025-08-04T14:52:11.070840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:16.162309Z",
     "iopub.status.busy": "2025-08-04T14:52:16.162001Z",
     "iopub.status.idle": "2025-08-04T14:52:16.822549Z",
     "shell.execute_reply": "2025-08-04T14:52:16.821621Z",
     "shell.execute_reply.started": "2025-08-04T14:52:16.162281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64fbda7a1b9412eac66a5f0378a5995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=4):   0%|          | 0/2630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset standardized for Gemma-3 format\n"
     ]
    }
   ],
   "source": [
    "# Set the chat template (this is the missing step)\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "# Standardize your existing dataset\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)\n",
    "\n",
    "print(\"‚úÖ Dataset standardized for Gemma-3 format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:16.823780Z",
     "iopub.status.busy": "2025-08-04T14:52:16.823555Z",
     "iopub.status.idle": "2025-08-04T14:52:17.545774Z",
     "shell.execute_reply": "2025-08-04T14:52:17.545245Z",
     "shell.execute_reply.started": "2025-08-04T14:52:16.823756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c627acbcb164c31945c17ae2e76fa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:17.546645Z",
     "iopub.status.busy": "2025-08-04T14:52:17.546454Z",
     "iopub.status.idle": "2025-08-04T14:52:22.885837Z",
     "shell.execute_reply": "2025-08-04T14:52:22.884910Z",
     "shell.execute_reply.started": "2025-08-04T14:52:17.546630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d28db0ea654d0b8d51b2c9f9aaca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cfb7db60fd404b963896dc8ce66efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Restart trainer with smaller batch size\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,  # Reduce from 4 to 1\n",
    "        gradient_accumulation_steps = 8,  # Increase to maintain effective batch size\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 200,\n",
    "        # max_steps=10,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "        output_dir = \"/kaggle/working/gemma-qa-model\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Apply response-only training (THIS IS THE KEY!)\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T14:52:22.887103Z",
     "iopub.status.busy": "2025-08-04T14:52:22.886874Z",
     "iopub.status.idle": "2025-08-04T15:30:00.801806Z",
     "shell.execute_reply": "2025-08-04T15:30:00.801189Z",
     "shell.execute_reply.started": "2025-08-04T14:52:22.887079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,630 | Num Epochs = 2 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 19,210,240 of 7,869,188,432 (0.24% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 35:42, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.740100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.227200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.6901607799530027, metrics={'train_runtime': 2254.4004, 'train_samples_per_second': 1.419, 'train_steps_per_second': 0.089, 'total_flos': 1.209133073437152e+16, 'train_loss': 2.6901607799530027})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-04T14:37:56.363114Z",
     "iopub.status.idle": "2025-08-04T14:37:56.363412Z",
     "shell.execute_reply": "2025-08-04T14:37:56.363286Z",
     "shell.execute_reply.started": "2025-08-04T14:37:56.363274Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q huggingface_hub\n",
    "# from huggingface_hub import login\n",
    "# Paste the token you showed in the screenshot\n",
    "# # model = FastModel.for_inference(model)\n",
    "# model.push_to_hub_merged(\"dataakash/gemma-family-assistant-gguf\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:30:00.802756Z",
     "iopub.status.busy": "2025-08-04T15:30:00.802543Z",
     "iopub.status.idle": "2025-08-04T15:33:48.007740Z",
     "shell.execute_reply": "2025-08-04T15:33:48.007080Z",
     "shell.execute_reply.started": "2025-08-04T15:30:00.802740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcf64f61a1340678d80c0f438e7be81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7e1ce8d0374067af196d738198a6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db465adde81847af9566a04a792bfd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3748c1be9204b4c87ef15f2954384ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78520efbcbe4248846299ca9dbdf623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbd8fe31f1a4c6ca6a098f0c5276d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772bdf3555dc4a99bcf6e08b7fbddffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:37<01:53, 37.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66a772c76034c28a4118d7626fe8ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2316a0b3024be886dc900a8099df2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddb46e3c57c4daaab7091cb13d20efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [01:45<01:50, 55.20s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544bfd16680b4eefb88c75d0effb8d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b9b437a009473fadf760e040256c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1eb077336949f8aad9ec8aab8d6120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [02:57<01:02, 62.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f305b24122ae469eac7a925579e212ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bab5ae58d9423284cc53fbd9b90269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2a650bec06483badfd7949dd004c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:33<00:00, 53.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "model.push_to_hub_merged(\"dataakash/gemma-family-assistant-gguf\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:44:47.533326Z",
     "iopub.status.busy": "2025-08-04T15:44:47.532727Z",
     "iopub.status.idle": "2025-08-04T15:47:34.624286Z",
     "shell.execute_reply": "2025-08-04T15:47:34.623583Z",
     "shell.execute_reply.started": "2025-08-04T15:44:47.533297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c377f0b0fb884965aa11ec88d1c4eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5201bef186b4b938ea35086678f756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:27<01:21, 27.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a38b6a320d4828a67412b96d7aabc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [01:22<01:27, 43.60s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687ca583cfc1433580a4f003374e3cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [02:08<00:44, 44.73s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e05a629d154eff955cea47f55d49c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:39<00:00, 39.75s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"/tmp/gemma-merged\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:50:34.506125Z",
     "iopub.status.busy": "2025-08-04T15:50:34.505244Z",
     "iopub.status.idle": "2025-08-04T15:50:34.952097Z",
     "shell.execute_reply": "2025-08-04T15:50:34.951288Z",
     "shell.execute_reply.started": "2025-08-04T15:50:34.506095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_template.jinja\t\t  model.safetensors.index.json\n",
      "config.json\t\t\t  preprocessor_config.json\n",
      "generation_config.json\t\t  processor_config.json\n",
      "model-00001-of-00004.safetensors  special_tokens_map.json\n",
      "model-00002-of-00004.safetensors  tokenizer_config.json\n",
      "model-00003-of-00004.safetensors  tokenizer.json\n",
      "model-00004-of-00004.safetensors  tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/gemma-merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:43:07.765929Z",
     "iopub.status.busy": "2025-08-04T15:43:07.765590Z",
     "iopub.status.idle": "2025-08-04T15:43:10.625244Z",
     "shell.execute_reply": "2025-08-04T15:43:10.624426Z",
     "shell.execute_reply.started": "2025-08-04T15:43:07.765902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 1543, done.\u001b[K\n",
      "remote: Counting objects: 100% (1543/1543), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1195/1195), done.\u001b[K\n",
      "remote: Total 1543 (delta 312), reused 1074 (delta 294), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (1543/1543), 23.89 MiB | 22.74 MiB/s, done.\n",
      "Resolving deltas: 100% (312/312), done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GIT_ASKPASS\"] = \"echo\"\n",
    "\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:47:34.627754Z",
     "iopub.status.busy": "2025-08-04T15:47:34.627352Z",
     "iopub.status.idle": "2025-08-04T15:47:34.682006Z",
     "shell.execute_reply": "2025-08-04T15:47:34.681278Z",
     "shell.execute_reply.started": "2025-08-04T15:47:34.627735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/llama.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T15:53:35.415924Z",
     "iopub.status.busy": "2025-08-04T15:53:35.414995Z",
     "iopub.status.idle": "2025-08-04T15:58:26.020749Z",
     "shell.execute_reply": "2025-08-04T15:58:26.019969Z",
     "shell.execute_reply.started": "2025-08-04T15:53:35.415893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.30G/7.30G [04:34<00:00, 26.6Mbyte/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/llama.cpp/convert_hf_to_gguf.py \\\n",
    "  --outfile /tmp/gemma-merged.gguf \\\n",
    "  --outtype q8_0 \\\n",
    "  /tmp/gemma-merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T16:04:14.119412Z",
     "iopub.status.busy": "2025-08-04T16:04:14.118713Z",
     "iopub.status.idle": "2025-08-04T16:04:14.668895Z",
     "shell.execute_reply": "2025-08-04T16:04:14.668287Z",
     "shell.execute_reply.started": "2025-08-04T16:04:14.119387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/dataakash/gemma-family-temp', endpoint='https://huggingface.co', repo_type='model', repo_id='dataakash/gemma-family-temp')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api = HfApi()\n",
    "api.create_repo(\n",
    "    repo_id=\"gemma-family-temp\",  # Use repo_id instead of name\n",
    "    private=True,\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T16:04:29.635211Z",
     "iopub.status.busy": "2025-08-04T16:04:29.634603Z",
     "iopub.status.idle": "2025-08-04T16:05:22.624842Z",
     "shell.execute_reply": "2025-08-04T16:05:22.624110Z",
     "shell.execute_reply.started": "2025-08-04T16:04:29.635186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7624866f9b9f4defaa457f2a990578e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddffd1f3153443392a344fcffd11137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-merged.gguf:   0%|          | 0.00/7.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dataakash/gemma-family-temp/commit/cd95b7fb5e79553d45eea8bed0261fa8aadeb2a2', commit_message='Upload gemma-merged.gguf with huggingface_hub', commit_description='', oid='cd95b7fb5e79553d45eea8bed0261fa8aadeb2a2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dataakash/gemma-family-temp', endpoint='https://huggingface.co', repo_type='model', repo_id='dataakash/gemma-family-temp'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/kaggle/working/gemma-merged.gguf\",\n",
    "    path_in_repo=\"gemma-merged.gguf\",\n",
    "    repo_id=\"dataakash/gemma-family-temp\",   # match above\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
